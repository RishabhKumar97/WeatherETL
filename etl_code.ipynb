{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4386f2d6",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313cb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from utilities import flatten_nested_dict\n",
    "import os\n",
    "import re\n",
    "import configparser\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84bbe3",
   "metadata": {},
   "source": [
    "**Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a838f4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_to_response_file_directory: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\\n"
     ]
    }
   ],
   "source": [
    "#pandas config for display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#base path\n",
    "base_path = os.getcwd()\n",
    "\n",
    "#read config file and get api key and locations\n",
    "config = configparser.ConfigParser(allow_no_value=True)\n",
    "config.read('cfg.ini')\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "locations = config.options('Locations')\n",
    "\n",
    "#path for base directory for the weather api response files\n",
    "path_to_response_file_directory = os.path.join(base_path, f\"weather_api_response_files{os.sep}\")\n",
    "print(\"path_to_response_file_directory:\", path_to_response_file_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea528a9",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4209d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get geo codes for locations through api\n",
    "def get_codes_for_location(location:list|str)->pd.DataFrame:\n",
    "    geo_codes_location_dict = {\n",
    "        'name': [],\n",
    "        'lat': [],\n",
    "        'lon': [],\n",
    "        'country': [],  \n",
    "        'state': []\n",
    "    }\n",
    "    if(type(location) == str):\n",
    "        location = [location]\n",
    "    for each in location: \n",
    "        geo_coding_api_url = f\"http://api.openweathermap.org/geo/1.0/direct?q={each}&limit=5&appid={API_KEY}\"\n",
    "\n",
    "        geo_code_response = requests.get(geo_coding_api_url)\n",
    "\n",
    "        if geo_code_response.status_code != 200:\n",
    "            print(\"Error\")\n",
    "            return None\n",
    "        else:\n",
    "            geo_code_response = geo_code_response.content\n",
    "            #manual decoding for byte literal response and converting it into a dict .decode didn't work\n",
    "            new_geo_code_response = eval(re.sub(\"^b[']|[']$\", \"\", str(geo_code_response).replace(r\"\\x\", \"\")))\n",
    "            geo_codes_location_dict['name'].append(new_geo_code_response[0]['name'])\n",
    "            geo_codes_location_dict['lat'].append(new_geo_code_response[0]['lat'])\n",
    "            geo_codes_location_dict['lon'].append(new_geo_code_response[0]['lon'])\n",
    "            geo_codes_location_dict['country'].append(new_geo_code_response[0]['country'])\n",
    "            geo_codes_location_dict['state'].append(new_geo_code_response[0]['state'])\n",
    "        \n",
    "        geo_codes_df = pd.DataFrame(geo_codes_location_dict)\n",
    "    return geo_codes_df\n",
    "\n",
    "def get_weather_data_and_write_to_csv(geo_codes_df: pd.DataFrame):\n",
    "    select_cols = ['coord_lon', 'coord_lat', 'weather_0_main', 'weather_0_description', 'main_temp','main_feels_like','main_temp_min','main_temp_max'\\\n",
    "            ,'main_pressure','main_humidity','main_sea_level','main_grnd_level','visibility','wind_speed','wind_deg','wind_gust','clouds_all'\\\n",
    "            , 'dt','sys_country','sys_sunrise','sys_sunset','timezone','name']\n",
    "    #ensure base path exists\n",
    "    os.makedirs(path_to_response_file_directory, exist_ok= True)\n",
    "    for each in geo_codes_df.itertuples():\n",
    "        lon = each.lon\n",
    "        lat = each.lat\n",
    "        name = each.name\n",
    "        country = each.country\n",
    "        state = each.state\n",
    "        # weather data api url\n",
    "        weather_data_api =f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={API_KEY}\"\n",
    "        #decode and store the response\n",
    "        weather_data_api_response = eval(requests.get(weather_data_api).content.decode('utf-8'))\n",
    "        # flatten and create df\n",
    "        df_weather_data = pd.DataFrame(flatten_nested_dict(weather_data_api_response), index= [0])\n",
    "        df_weather_data_select_cols = df_weather_data[select_cols]\n",
    "        # print(df_weather_data.dtypes)\n",
    "        #ensure file path exists for writing the df\n",
    "        file_path = f\"{country}{os.sep}{state}{os.sep}\"\n",
    "        full_path = os.path.join(path_to_response_file_directory, file_path)\n",
    "        os.makedirs(full_path, exist_ok= True)\n",
    "        # save the df into a file\n",
    "        print(f\"Inserting raw data to path: {full_path}\", f\"file name: weather_data_{name.lower()}.csv\")\n",
    "        # Write to CSV, include header only if the file doesn't exist\n",
    "        df_weather_data_select_cols.to_csv(f'{full_path}weather_data_{name.lower()}.csv', sep= '|', index= False, mode = 'a', header= not(os.path.isfile(f'{full_path}weather_data_{name.lower()}.csv')))\n",
    "\n",
    "def get_transformed_data(path_to_data:str)->pd.DataFrame:\n",
    "    select_cols = ['coord_lon', 'coord_lat', 'weather_0_main', 'weather_0_description', 'main_temp','main_feels_like','main_temp_min','main_temp_max'\\\n",
    "            ,'main_pressure','main_humidity','main_sea_level','main_grnd_level','visibility','wind_speed','wind_deg','wind_gust','clouds_all'\\\n",
    "            , 'dt','sys_country','sys_sunrise','sys_sunset','timezone','name']\n",
    "    unix_time_cols = ['dt','sys_sunrise','sys_sunset']\n",
    "    local_time_cols = ['dt_local','sys_sunrise_local','sys_sunset_local']\n",
    "    col_rep_shift_utc = ['timezone']\n",
    "    df = pd.read_csv(path_to_data, usecols= select_cols, delimiter= '|')\n",
    "    df.drop_duplicates(subset = ['dt'], keep= 'last', inplace= True, ignore_index= True)\n",
    "    #local time\n",
    "    for i in range(0,3):\n",
    "        df[local_time_cols[i]] = df[unix_time_cols[i]] + df[col_rep_shift_utc[0]]\n",
    "        df[local_time_cols[i]] = pd.to_datetime(df[local_time_cols[i]], unit= 's')\n",
    "    #UTC time\n",
    "    for c in unix_time_cols:\n",
    "        df[c+\"_utc\"] = pd.to_datetime(df[c], unit= 's')\n",
    "    df['state'] = (path_to_data.split('weather_api_response_files')[1]).split(os.sep)[2]\n",
    "    # df['etl_timestamp'] = pd.Timestamp.now()\n",
    "    # df['location'] = (((path_to_data.split('weather_api_response_files')[1]).split(os.sep)[3]).split(\"_\")[2]).split(\".\")[0]\n",
    "    df.drop(columns= unix_time_cols, axis=1, inplace=True)\n",
    "    df.drop(columns= ['timezone'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def conn_to_postgres_db(database):\n",
    "    with open(r\"C:\\Users\\risha\\python_to_postgreSQL_etl\\python_to_postgreSQL_etl\\Creds\\postgres_db.json\", 'r') as fp:\n",
    "        pg = json.load(fp)\n",
    "    db_connect_str = f\"postgresql+psycopg2://{pg[0]['user']}:{pg[0]['pass']}@{pg[0]['host']}:{pg[0]['port']}/{[db for db in pg[0]['db'] if db == database][0]}\"\n",
    "    conn = create_engine(db_connect_str)\n",
    "    return conn\n",
    "\n",
    "def get_max_etl_timestamp(conn):\n",
    "    cur = conn.connect()\n",
    "    t = cur.execute(text(f\"select max(etl_timestamp) from weather_data_raw \")).fetchone()\n",
    "    return t[0]\n",
    "\n",
    "def push_to_postgres_db(conn):\n",
    "    for d, sd, files in os.walk(os.path.join(base_path, f'weather_api_response_files{os.sep}')):\n",
    "        for f in files:\n",
    "            print(f\"Pushing the file {f} into the weather_data_raw table\")\n",
    "            df = get_transformed_data(os.path.join(d, f))\n",
    "            # try:\n",
    "            #     t = get_max_etl_timestamp(conn)   \n",
    "            # except:\n",
    "            #     df.to_sql(name='weather_data_raw', con= conn, if_exists='append', index= False)\n",
    "            # t = get_max_etl_timestamp(conn)\n",
    "            # df = df[df['etl_timestamp'] > t]\n",
    "            df.to_sql(name='weather_data_raw', con= conn, if_exists='append', index= False)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b5786",
   "metadata": {},
   "source": [
    "**Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6abd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\IN\\Delhi\\ file name: weather_data_delhi.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\IN\\Maharashtra\\ file name: weather_data_mumbai.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\US\\Texas\\ file name: weather_data_dallas.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\US\\New York\\ file name: weather_data_new york.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\IN\\Tamil Nadu\\ file name: weather_data_chennai.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\PH\\Camarines Sur\\ file name: weather_data_goa.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\US\\Florida\\ file name: weather_data_miami.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\US\\Missouri\\ file name: weather_data_california.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\US\\New York\\ file name: weather_data_manhattan.csv\n",
      "Inserting raw data to path: c:\\Users\\risha\\WeatherETL\\weather_api_response_files\\IN\\Uttar Pradesh\\ file name: weather_data_noida.csv\n"
     ]
    }
   ],
   "source": [
    "if(__name__ == \"__main__\"):\n",
    "    for loc in locations:\n",
    "        geo_codes_df = get_codes_for_location(location= loc)\n",
    "        path_to_data = get_weather_data_and_write_to_csv(geo_codes_df)\n",
    "    # conn = conn_to_postgres_db('weather_data_db')\n",
    "    # push_to_postgres_db(conn)\n",
    "    \n",
    "\n",
    "\n",
    "    # s3_client = boto3.client('s3')\n",
    "    # Bucket = boto3.resource('s3').Bucket(\"xyz-a8cf379f-3db9-4fe4-a9d6-1e169a3d9353\")\n",
    "    # bulk_load_raw_to_s3(path_to_response_file_directory, Bucket)\n",
    "    # print(check_bucket_exists(\"xyz-a8cf379f-3db9-4fe4-a9d6-1e169a3d9353\", s3_client))\n",
    "    # # print((s3_client.list_buckets()))\n",
    "    # print(path_to_response_file_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
